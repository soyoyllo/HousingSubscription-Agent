import os
import asyncio
import hashlib
import networkx as nx
import numpy as np
from datetime import datetime, timedelta
from dataclasses import dataclass, field
from typing import Dict, List, Optional, Set, Tuple, Any
from collections import defaultdict, Counter
import logging
import pickle
import json
import re

# ML/NLP Libraries
from sklearn.cluster import DBSCAN
from sklearn.metrics.pairwise import cosine_similarity
from sklearn.feature_extraction.text import TfidfVectorizer
import pandas as pd

# LangChain & OpenAI
from langchain_openai import OpenAIEmbeddings, ChatOpenAI
from langchain_community.vectorstores import Chroma
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain_core.documents import Document

# ê¸°ì¡´ Notice RAG ëª¨ë“ˆ
from notice_rag_pipeline import NoticeRAGPipeline, NoticeInfo

# ==================== ë°ì´í„° ëª¨ë¸ ====================

@dataclass
class KnowledgeNode:
    """ì§€ì‹ ê·¸ë˜í”„ ë…¸ë“œ"""
    node_id: str
    node_type: str  # concept, rule, procedure, requirement
    content: str
    confidence_score: float  # 0.0 ~ 1.0
    source_count: int  # ì´ ì§€ì‹ì´ ë‚˜íƒ€ë‚œ ì†ŒìŠ¤ ìˆ˜
    validation_count: int  # ê²€ì¦ëœ íšŸìˆ˜
    created_at: datetime
    last_updated: datetime
    
    # ë©”íƒ€ë°ì´í„°
    applicable_types: Set[str] = field(default_factory=set)  # ì ìš© ê°€ëŠ¥í•œ ì²­ì•½ ìœ í˜•ë“¤
    regional_scope: Set[str] = field(default_factory=set)    # ì ìš© ì§€ì—­ ë²”ìœ„
    temporal_validity: Optional[str] = None  # ì‹œê°„ì  ìœ íš¨ì„±
    evidence_sources: List[str] = field(default_factory=list)  # ì¦ê±° ì†ŒìŠ¤ë“¤

@dataclass
class KnowledgeEdge:
    """ì§€ì‹ ê·¸ë˜í”„ ì—£ì§€"""
    source_node: str
    target_node: str
    relation_type: str  # requires, implies, contradicts, similar_to
    weight: float       # ê´€ê³„ ê°•ë„ (0.0 ~ 1.0)
    evidence_count: int # ì´ ê´€ê³„ë¥¼ ë’·ë°›ì¹¨í•˜ëŠ” ì¦ê±° ìˆ˜
    created_at: datetime

@dataclass
class CommonPattern:
    """ë°œê²¬ëœ ê³µí†µ íŒ¨í„´"""
    pattern_id: str
    pattern_type: str  # ìê²©ìš”ê±´, ì‹ ì²­ì ˆì°¨, ë¹„ìš©ì •ë³´ ë“±
    content: str
    frequency: int           # ë°œê²¬ ë¹ˆë„
    confidence: float        # ì‹ ë¢°ë„ (0.0 ~ 1.0)
    source_notices: List[str] # ì´ íŒ¨í„´ì´ ë‚˜íƒ€ë‚œ ê³µê³ ë¬¸ë“¤
    ì²­ì•½ìœ í˜•: str              # ì²­ë…„ì£¼íƒ, í–‰ë³µì£¼íƒ ë“±
    extracted_at: datetime

@dataclass
class QueryRecord:
    """ì§ˆì˜ ê¸°ë¡"""
    query_id: str
    user_query: str
    response: str
    notice_info: Optional[NoticeInfo]
    confidence: float
    timestamp: datetime
    query_embedding: Optional[np.ndarray] = None
    response_embedding: Optional[np.ndarray] = None

@dataclass
class KnowledgeCandidate:
    """ì§€ì‹ í›„ë³´"""
    candidate_id: str
    query_type: str
    representative_query: str
    representative_response: str
    applicable_types: List[str]
    evidence_count: int
    confidence: float
    first_seen: datetime
    last_seen: datetime
    validation_needed: bool = True
    supporting_queries: List[str] = field(default_factory=list)

# ==================== íŒ¨í„´ ë°œê²¬ ì—”ì§„ ====================

class PatternExtractor:
    """ê¸°ë³¸ íŒ¨í„´ ì¶”ì¶œê¸° ì¸í„°í˜ì´ìŠ¤"""
    
    def __init__(self, pattern_type: str):
        self.pattern_type = pattern_type
        self.rag_pipeline = None
    
    async def extract(self, notice: NoticeInfo) -> Optional[str]:
        """ê³µê³ ë¬¸ì—ì„œ íŠ¹ì • ìœ í˜•ì˜ ì •ë³´ ì¶”ì¶œ"""
        raise NotImplementedError

class QualificationPatternExtractor(PatternExtractor):
    """ìê²© ìš”ê±´ íŒ¨í„´ ì¶”ì¶œê¸°"""
    
    def __init__(self):
        super().__init__("ìê²©ìš”ê±´")
        
    async def extract(self, notice: NoticeInfo) -> Optional[str]:
        """ê³µê³ ë¬¸ì—ì„œ ìê²© ìš”ê±´ ì •ë³´ ì¶”ì¶œ"""
        
        if not self.rag_pipeline:
            from notice_rag_pipeline import RAGConfig
            self.rag_pipeline = NoticeRAGPipeline(RAGConfig())
        
        try:
            # Notice RAGë¥¼ í™œìš©í•˜ì—¬ ìê²© ìš”ê±´ ì¶”ì¶œ
            response = self.rag_pipeline.query_notice(notice, "ìê²© ìš”ê±´ì´ ë¬´ì—‡ì¸ê°€ìš”?")
            
            if response and 'answer' in response:
                # êµ¬ì¡°í™”ëœ ì •ë³´ ì¶”ì¶œ
                structured_info = self._parse_qualification_info(response['answer'])
                return structured_info
                
        except Exception as e:
            logging.error(f"ìê²© ìš”ê±´ ì¶”ì¶œ ì‹¤íŒ¨ ({notice.ê³µê³ ëª…}): {str(e)}")
            
        return None
    
    def _parse_qualification_info(self, raw_response: str) -> str:
        """ìê²© ìš”ê±´ ì‘ë‹µì—ì„œ êµ¬ì¡°í™”ëœ ì •ë³´ ì¶”ì¶œ"""
        
        # ì£¼ìš” ìê²© ìš”ê±´ íŒ¨í„´ë“¤
        patterns = {
            "ì—°ë ¹": r'ë§Œ\s*(\d+)ì„¸?\s*(?:ì´ìƒ\s*)?(?:~|-)?\s*(?:ë§Œ\s*)?(\d+)ì„¸?\s*ì´í•˜',
            "ì†Œë“": r'(?:ì›”í‰ê· ì†Œë“|ì†Œë“)\s*(\d+)%\s*ì´í•˜',
            "ê±°ì£¼": r'(ì„œìš¸|ê²½ê¸°|ì¸ì²œ|ë¶€ì‚°|ëŒ€êµ¬|ê´‘ì£¼|ëŒ€ì „|ìš¸ì‚°|ì„¸ì¢…|ì œì£¼).*?(?:ê±°ì£¼|ì§ì¥)',
            "ë¬´ì£¼íƒ": r'ë¬´ì£¼íƒ.*?(?:ì„¸ëŒ€êµ¬ì„±ì›|ì„¸ëŒ€ì£¼|ê°€êµ¬ì›)',
            "í˜¼ì¸": r'(ë¯¸í˜¼|ê¸°í˜¼|í˜¼ì¸|ì‹ í˜¼ë¶€ë¶€)'
        }
        
        extracted_info = {}
        
        for category, pattern in patterns.items():
            matches = re.findall(pattern, raw_response, re.IGNORECASE)
            if matches:
                if category == "ì—°ë ¹" and len(matches[0]) == 2:
                    extracted_info[category] = f"{matches[0][0]}~{matches[0][1]}ì„¸"
                elif category == "ì†Œë“":
                    extracted_info[category] = f"{matches[0]}% ì´í•˜"
                else:
                    extracted_info[category] = str(matches[0])
        
        return json.dumps(extracted_info, ensure_ascii=False)

class ProcedurePatternExtractor(PatternExtractor):
    """ì‹ ì²­ ì ˆì°¨ íŒ¨í„´ ì¶”ì¶œê¸°"""
    
    def __init__(self):
        super().__init__("ì‹ ì²­ì ˆì°¨")
        
    async def extract(self, notice: NoticeInfo) -> Optional[str]:
        """ê³µê³ ë¬¸ì—ì„œ ì‹ ì²­ ì ˆì°¨ ì •ë³´ ì¶”ì¶œ"""
        
        if not self.rag_pipeline:
            from notice_rag_pipeline import RAGConfig
            self.rag_pipeline = NoticeRAGPipeline(RAGConfig())
        
        try:
            response = self.rag_pipeline.query_notice(notice, "ì‹ ì²­ ë°©ë²•ê³¼ ì ˆì°¨ë¥¼ ì•Œë ¤ì£¼ì„¸ìš”")
            
            if response and 'answer' in response:
                structured_info = self._parse_procedure_info(response['answer'])
                return structured_info
                
        except Exception as e:
            logging.error(f"ì‹ ì²­ ì ˆì°¨ ì¶”ì¶œ ì‹¤íŒ¨ ({notice.ê³µê³ ëª…}): {str(e)}")
            
        return None
    
    def _parse_procedure_info(self, raw_response: str) -> str:
        """ì‹ ì²­ ì ˆì°¨ ì‘ë‹µì—ì„œ êµ¬ì¡°í™”ëœ ì •ë³´ ì¶”ì¶œ"""
        
        # ì ˆì°¨ ë‹¨ê³„ ì¶”ì¶œ
        step_patterns = [
            r'(\d+)\.\s*([^0-9\n]+)',  # 1. ì˜¨ë¼ì¸ ì ‘ìˆ˜
            r'[â‘ â‘¡â‘¢â‘£â‘¤â‘¥â‘¦â‘§â‘¨â‘©]\s*([^â‘ â‘¡â‘¢â‘£â‘¤â‘¥â‘¦â‘§â‘¨â‘©\n]+)',  # â‘  ì˜¨ë¼ì¸ ì ‘ìˆ˜
            r'(?:ì²«\s*ë²ˆì§¸|ë‘\s*ë²ˆì§¸|ì„¸\s*ë²ˆì§¸|ë„¤\s*ë²ˆì§¸|ë‹¤ì„¯\s*ë²ˆì§¸)\s*[:ï¼š]?\s*([^\n]+)'
        ]
        
        steps = []
        for pattern in step_patterns:
            matches = re.findall(pattern, raw_response)
            if matches:
                for match in matches:
                    if isinstance(match, tuple):
                        steps.append(match[1].strip())
                    else:
                        steps.append(match.strip())
                break
        
        procedure_info = {
            "ë‹¨ê³„ìˆ˜": len(steps),
            "ì ˆì°¨": steps[:5]  # ìµœëŒ€ 5ë‹¨ê³„ê¹Œì§€
        }
        
        return json.dumps(procedure_info, ensure_ascii=False)

class CostPatternExtractor(PatternExtractor):
    """ë¹„ìš© ì •ë³´ íŒ¨í„´ ì¶”ì¶œê¸°"""
    
    def __init__(self):
        super().__init__("ë¹„ìš©ì •ë³´")
        
    async def extract(self, notice: NoticeInfo) -> Optional[str]:
        """ê³µê³ ë¬¸ì—ì„œ ë¹„ìš© ì •ë³´ ì¶”ì¶œ"""
        
        if not self.rag_pipeline:
            from notice_rag_pipeline import RAGConfig
            self.rag_pipeline = NoticeRAGPipeline(RAGConfig())
        
        try:
            response = self.rag_pipeline.query_notice(notice, "ì„ëŒ€ë£Œì™€ ë³´ì¦ê¸ˆì´ ì–¼ë§ˆì¸ê°€ìš”?")
            
            if response and 'answer' in response:
                structured_info = self._parse_cost_info(response['answer'])
                return structured_info
                
        except Exception as e:
            logging.error(f"ë¹„ìš© ì •ë³´ ì¶”ì¶œ ì‹¤íŒ¨ ({notice.ê³µê³ ëª…}): {str(e)}")
            
        return None
    
    def _parse_cost_info(self, raw_response: str) -> str:
        """ë¹„ìš© ì •ë³´ ì‘ë‹µì—ì„œ êµ¬ì¡°í™”ëœ ì •ë³´ ì¶”ì¶œ"""
        
        cost_patterns = {
            "ì„ëŒ€ë£Œ_ì‹œì„¸ë¹„ìœ¨": r'ì‹œì„¸.*?(\d+)%',
            "ì„ëŒ€ë£Œ_ê¸ˆì•¡": r'ì„ëŒ€ë£Œ.*?(\d+(?:,\d+)*).*?ì›',
            "ë³´ì¦ê¸ˆ": r'ë³´ì¦ê¸ˆ.*?(\d+(?:,\d+)*).*?ì›',
            "ê´€ë¦¬ë¹„": r'ê´€ë¦¬ë¹„.*?(\d+(?:,\d+)*).*?ì›'
        }
        
        cost_info = {}
        
        for cost_type, pattern in cost_patterns.items():
            matches = re.findall(pattern, raw_response)
            if matches:
                cost_info[cost_type] = matches[0]
        
        return json.dumps(cost_info, ensure_ascii=False)

# ==================== íŒ¨í„´ ë°œê²¬ ì—”ì§„ ====================

class PatternDiscoveryEngine:
    """ê³µê³ ë¬¸ì—ì„œ ê³µí†µ íŒ¨í„´ ìë™ ë°œê²¬"""
    
    def __init__(self):
        self.pattern_extractors = {
            "ìê²©ìš”ê±´": QualificationPatternExtractor(),
            "ì‹ ì²­ì ˆì°¨": ProcedurePatternExtractor(),
            "ë¹„ìš©ì •ë³´": CostPatternExtractor()
        }
        self.similarity_threshold = 0.85
        self.embeddings = OpenAIEmbeddings(model="text-embedding-3-small")
        
    async def discover_patterns_from_notices(self, 
                                           notices: List[NoticeInfo]) -> List[CommonPattern]:
        """ê³µê³ ë¬¸ë“¤ì—ì„œ ê³µí†µ íŒ¨í„´ ë°œê²¬"""
        
        logging.info(f"íŒ¨í„´ ë°œê²¬ ì‹œì‘: {len(notices)}ê°œ ê³µê³ ë¬¸ ë¶„ì„")
        
        discovered_patterns = []
        
        # ì²­ì•½ ìœ í˜•ë³„ë¡œ ê·¸ë£¹í•‘
        grouped_notices = self._group_by_subscription_type(notices)
        
        for subscription_type, type_notices in grouped_notices.items():
            if len(type_notices) >= 3:  # ìµœì†Œ 3ê°œ ì´ìƒì¼ ë•Œë§Œ íŒ¨í„´ ë¶„ì„
                logging.info(f"{subscription_type}: {len(type_notices)}ê°œ ê³µê³ ë¬¸ì—ì„œ íŒ¨í„´ ë¶„ì„")
                
                patterns = await self._extract_common_patterns(
                    type_notices, subscription_type
                )
                discovered_patterns.extend(patterns)
        
        logging.info(f"ì´ {len(discovered_patterns)}ê°œ íŒ¨í„´ ë°œê²¬")
        return discovered_patterns
    
    def _group_by_subscription_type(self, notices: List[NoticeInfo]) -> Dict[str, List[NoticeInfo]]:
        """ì²­ì•½ ìœ í˜•ë³„ë¡œ ê³µê³ ë¬¸ ê·¸ë£¹í•‘"""
        
        groups = defaultdict(list)
        
        for notice in notices:
            # ì²­ì•½ìœ í˜•ì´ ì—†ìœ¼ë©´ ê³µê³ ëª…ì—ì„œ ì¶”ì •
            subscription_type = notice.ì²­ì•½ìœ í˜• or self._infer_subscription_type(notice.ê³µê³ ëª…)
            groups[subscription_type].append(notice)
        
        return dict(groups)
    
    def _infer_subscription_type(self, notice_title: str) -> str:
        """ê³µê³ ëª…ì—ì„œ ì²­ì•½ ìœ í˜• ì¶”ì •"""
        
        type_keywords = {
            "ì²­ë…„ì£¼íƒ": ["ì²­ë…„ì£¼íƒ", "ì²­ë…„", "youth"],
            "í–‰ë³µì£¼íƒ": ["í–‰ë³µì£¼íƒ", "í–‰ë³µ"],
            "ì „ì„¸ì„ëŒ€": ["ì „ì„¸ì„ëŒ€", "ì „ì„¸ì§€ì›"],
            "ë§¤ì…ì„ëŒ€": ["ë§¤ì…ì„ëŒ€"],
            "ê±´ì„¤ì„ëŒ€": ["ê±´ì„¤ì„ëŒ€"],
            "ì•ˆì‹¬ì£¼íƒ": ["ì•ˆì‹¬ì£¼íƒ", "ì•ˆì‹¬"]
        }
        
        title_lower = notice_title.lower()
        
        for subscription_type, keywords in type_keywords.items():
            if any(keyword in title_lower for keyword in keywords):
                return subscription_type
        
        return "ê¸°íƒ€"
    
    async def _extract_common_patterns(self, 
                                     notices: List[NoticeInfo], 
                                     subscription_type: str) -> List[CommonPattern]:
        """íŠ¹ì • ìœ í˜• ê³µê³ ë¬¸ë“¤ì—ì„œ ê³µí†µ íŒ¨í„´ ì¶”ì¶œ"""
        
        all_patterns = []
        
        for pattern_type, extractor in self.pattern_extractors.items():
            logging.info(f"{subscription_type} - {pattern_type} íŒ¨í„´ ì¶”ì¶œ ì¤‘...")
            
            # ê° ê³µê³ ë¬¸ì—ì„œ í•´ë‹¹ ìœ í˜•ì˜ ì •ë³´ ì¶”ì¶œ
            extracted_info = []
            for notice in notices:
                try:
                    info = await extractor.extract(notice)
                    if info:
                        extracted_info.append({
                            'notice_id': notice.ê³µê³ ëª…,
                            'content': info,
                            'notice': notice
                        })
                except Exception as e:
                    logging.warning(f"ì •ë³´ ì¶”ì¶œ ì‹¤íŒ¨ ({notice.ê³µê³ ëª…}): {str(e)}")
                    continue
            
            # ê³µí†µ íŒ¨í„´ ì°¾ê¸°
            if len(extracted_info) >= 2:
                common_patterns = await self._find_common_patterns(
                    extracted_info, pattern_type, subscription_type
                )
                all_patterns.extend(common_patterns)
        
        return all_patterns
    
    async def _find_common_patterns(self, 
                                  extracted_info: List[Dict], 
                                  pattern_type: str,
                                  subscription_type: str) -> List[CommonPattern]:
        """ì¶”ì¶œëœ ì •ë³´ì—ì„œ ê³µí†µ íŒ¨í„´ ì‹ë³„"""
        
        if len(extracted_info) < 2:
            return []
        
        patterns = []
        
        # ë‚´ìš©ë“¤ì„ ì„ë² ë”©ìœ¼ë¡œ ë³€í™˜
        contents = [item['content'] for item in extracted_info]
        
        try:
            # ì„ë² ë”© ìƒì„±
            embeddings = await self._get_embeddings(contents)
            
            # í´ëŸ¬ìŠ¤í„°ë§ìœ¼ë¡œ ìœ ì‚¬í•œ ë‚´ìš© ê·¸ë£¹í•‘
            clusters = self._cluster_similar_content(embeddings)
            
            for cluster_indices in clusters:
                if len(cluster_indices) >= 2:  # 2ê°œ ì´ìƒì˜ ê³µê³ ë¬¸ì—ì„œ ë‚˜íƒ€ë‚˜ëŠ” íŒ¨í„´
                    
                    cluster_contents = [contents[i] for i in cluster_indices]
                    cluster_notices = [extracted_info[i]['notice_id'] for i in cluster_indices]
                    
                    # í´ëŸ¬ìŠ¤í„° ë‚´ ê°€ì¥ ëŒ€í‘œì ì¸ ë‚´ìš© ì„ íƒ
                    representative_content = self._select_representative_content(cluster_contents)
                    
                    # íŒ¨í„´ ìƒì„±
                    pattern_id = f"{subscription_type}_{pattern_type}_{abs(hash(representative_content))}"
                    
                    pattern = CommonPattern(
                        pattern_id=pattern_id,
                        pattern_type=pattern_type,
                        content=representative_content,
                        frequency=len(cluster_indices),
                        confidence=len(cluster_indices) / len(extracted_info),
                        source_notices=cluster_notices,
                        ì²­ì•½ìœ í˜•=subscription_type,
                        extracted_at=datetime.now()
                    )
                    
                    patterns.append(pattern)
                    logging.info(f"íŒ¨í„´ ë°œê²¬: {pattern_id} (ë¹ˆë„: {pattern.frequency})")
        
        except Exception as e:
            logging.error(f"íŒ¨í„´ ì°¾ê¸° ì‹¤íŒ¨: {str(e)}")
        
        return patterns
    
    async def _get_embeddings(self, texts: List[str]) -> np.ndarray:
        """í…ìŠ¤íŠ¸ë“¤ì˜ ì„ë² ë”© ìƒì„±"""
        
        try:
            embeddings = self.embeddings.embed_documents(texts)
            return np.array(embeddings)
        except Exception as e:
            logging.error(f"ì„ë² ë”© ìƒì„± ì‹¤íŒ¨: {str(e)}")
            # ì‹¤íŒ¨ ì‹œ TF-IDFë¡œ ëŒ€ì²´
            vectorizer = TfidfVectorizer(max_features=100)
            return vectorizer.fit_transform(texts).toarray()
    
    def _cluster_similar_content(self, embeddings: np.ndarray) -> List[List[int]]:
        """ì„ë² ë”© ê¸°ë°˜ í´ëŸ¬ìŠ¤í„°ë§"""
        
        if len(embeddings) < 2:
            return []
        
        # DBSCAN í´ëŸ¬ìŠ¤í„°ë§
        clustering = DBSCAN(eps=0.3, min_samples=2, metric='cosine')
        cluster_labels = clustering.fit_predict(embeddings)
        
        # í´ëŸ¬ìŠ¤í„°ë³„ë¡œ ì¸ë±ìŠ¤ ê·¸ë£¹í•‘
        clusters = defaultdict(list)
        for idx, label in enumerate(cluster_labels):
            if label != -1:  # ë…¸ì´ì¦ˆê°€ ì•„ë‹Œ ê²½ìš°ë§Œ
                clusters[label].append(idx)
        
        return list(clusters.values())
    
    def _select_representative_content(self, contents: List[str]) -> str:
        """í´ëŸ¬ìŠ¤í„°ì—ì„œ ê°€ì¥ ëŒ€í‘œì ì¸ ë‚´ìš© ì„ íƒ"""
        
        if len(contents) == 1:
            return contents[0]
        
        # ê°€ì¥ ê¸´ ë‚´ìš©ì„ ëŒ€í‘œë¡œ ì„ íƒ (ì •ë³´ê°€ ê°€ì¥ ì™„ì „í•  ê°€ëŠ¥ì„±)
        return max(contents, key=len)

# ==================== ì§ˆì˜ íŒ¨í„´ ë¶„ì„ ì‹œìŠ¤í…œ ====================

class QueryPatternAnalyzer:
    """ì‚¬ìš©ì ì§ˆì˜ íŒ¨í„´ ë¶„ì„ ë° ë³´í¸ì  ì§€ì‹ ë°œê²¬"""
    
    def __init__(self):
        self.query_history: List[QueryRecord] = []
        self.knowledge_candidates: Dict[str, KnowledgeCandidate] = {}
        self.embeddings = OpenAIEmbeddings(model="text-embedding-3-small")
        self.similarity_threshold = 0.85
        self.min_evidence_count = 3
        
    def record_query_response(self, 
                            user_query: str, 
                            response: str, 
                            notice_info: Optional[NoticeInfo] = None,
                            confidence: float = 0.8):
        """ì§ˆì˜-ì‘ë‹µ ê¸°ë¡ ì €ì¥"""
        
        query_id = hashlib.md5(f"{user_query}_{datetime.now()}".encode()).hexdigest()
        
        query_record = QueryRecord(
            query_id=query_id,
            user_query=user_query,
            response=response,
            notice_info=notice_info,
            confidence=confidence,
            timestamp=datetime.now()
        )
        
        self.query_history.append(query_record)
        
        # ì‹¤ì‹œê°„ íŒ¨í„´ ë¶„ì„
        asyncio.create_task(self._analyze_emerging_patterns(query_record))
        
        logging.info(f"ì§ˆì˜ ê¸°ë¡: {user_query[:50]}...")
    
    async def _analyze_emerging_patterns(self, new_record: QueryRecord):
        """ìƒˆë¡œìš´ ì§ˆì˜ì—ì„œ ë‚˜íƒ€ë‚˜ëŠ” íŒ¨í„´ ë¶„ì„"""
        
        try:
            # 1. ì¿¼ë¦¬ ì„ë² ë”© ìƒì„±
            if new_record.query_embedding is None:
                query_embedding = await self._get_query_embedding(new_record.user_query)
                new_record.query_embedding = query_embedding
            
            # 2. ìœ ì‚¬í•œ ì§ˆë¬¸ ì°¾ê¸°
            similar_queries = await self._find_similar_queries(new_record)
            
            if len(similar_queries) >= self.min_evidence_count:
                # 3. ì‘ë‹µ ì¼ê´€ì„± í™•ì¸
                all_responses = [q.response for q in similar_queries] + [new_record.response]
                consistency_score = await self._calculate_response_consistency(all_responses)
                
                if consistency_score >= 0.8:  # 80% ì´ìƒ ì¼ê´€ì„±
                    # 4. ë³´í¸ì  ì§€ì‹ í›„ë³´ë¡œ ë“±ë¡
                    await self._register_knowledge_candidate(new_record, similar_queries)
                    
        except Exception as e:
            logging.error(f"íŒ¨í„´ ë¶„ì„ ì‹¤íŒ¨: {str(e)}")
    
    async def _get_query_embedding(self, query: str) -> np.ndarray:
        """ì§ˆì˜ ì„ë² ë”© ìƒì„±"""
        
        try:
            embedding = self.embeddings.embed_query(query)
            return np.array(embedding)
        except Exception as e:
            logging.error(f"ì„ë² ë”© ìƒì„± ì‹¤íŒ¨: {str(e)}")
            return np.zeros(1536)  # OpenAI ì„ë² ë”© ê¸°ë³¸ ì°¨ì›
    
    async def _find_similar_queries(self, target_record: QueryRecord) -> List[QueryRecord]:
        """ìœ ì‚¬í•œ ì§ˆë¬¸ë“¤ ì°¾ê¸°"""
        
        similar_queries = []
        
        for record in self.query_history[:-1]:  # ë§ˆì§€ë§‰ ê¸°ë¡(ìì‹ ) ì œì™¸
            if record.query_embedding is None:
                record.query_embedding = await self._get_query_embedding(record.user_query)
            
            # ì½”ì‚¬ì¸ ìœ ì‚¬ë„ ê³„ì‚°
            similarity = self._cosine_similarity(
                target_record.query_embedding, 
                record.query_embedding
            )
            
            if similarity >= self.similarity_threshold:
                similar_queries.append(record)
        
        return similar_queries
    
    def _cosine_similarity(self, vec1: np.ndarray, vec2: np.ndarray) -> float:
        """ì½”ì‚¬ì¸ ìœ ì‚¬ë„ ê³„ì‚°"""
        
        try:
            return cosine_similarity([vec1], [vec2])[0][0]
        except:
            return 0.0
    
    async def _calculate_response_consistency(self, responses: List[str]) -> float:
        """ì‘ë‹µë“¤ ê°„ì˜ ì¼ê´€ì„± ì ìˆ˜ ê³„ì‚°"""
        
        if len(responses) < 2:
            return 0.0
        
        try:
            # ì‘ë‹µ ì„ë² ë”© ìƒì„±
            response_embeddings = []
            for response in responses:
                embedding = await self._get_query_embedding(response)
                response_embeddings.append(embedding)
            
            # ëª¨ë“  ìŒì˜ ìœ ì‚¬ë„ ê³„ì‚°
            similarities = []
            for i in range(len(response_embeddings)):
                for j in range(i + 1, len(response_embeddings)):
                    sim = self._cosine_similarity(response_embeddings[i], response_embeddings[j])
                    similarities.append(sim)
            
            return sum(similarities) / len(similarities) if similarities else 0.0
            
        except Exception as e:
            logging.error(f"ì¼ê´€ì„± ê³„ì‚° ì‹¤íŒ¨: {str(e)}")
            return 0.0
    
    async def _register_knowledge_candidate(self, 
                                          trigger_record: QueryRecord, 
                                          similar_queries: List[QueryRecord]):
        """ë³´í¸ì  ì§€ì‹ í›„ë³´ ë“±ë¡"""
        
        # ì§ˆë¬¸ ìœ í˜• ë¶„ë¥˜
        query_type = self._classify_query_type(trigger_record.user_query)
        
        # ì ìš© ë²”ìœ„ ë¶„ì„ (ì–´ë–¤ ì²­ì•½ ìœ í˜•ë“¤ì— ê³µí†µìœ¼ë¡œ ì ìš©ë˜ëŠ”ê°€)
        applicable_types = set()
        for query in similar_queries + [trigger_record]:
            if query.notice_info and query.notice_info.ì²­ì•½ìœ í˜•:
                applicable_types.add(query.notice_info.ì²­ì•½ìœ í˜•)
        
        # ëŒ€í‘œ ì‘ë‹µ ìƒì„± (ê°€ì¥ ì™„ì „í•˜ê³  ì •í™•í•œ ì‘ë‹µ ì„ íƒ)
        all_responses = [q.response for q in similar_queries + [trigger_record]]
        representative_response = self._select_best_response(all_responses)
        
        candidate_id = f"{query_type}_{abs(hash(representative_response))}"
        
        # ê¸°ì¡´ í›„ë³´ì™€ ì¤‘ë³µ í™•ì¸
        if candidate_id not in self.knowledge_candidates:
            
            self.knowledge_candidates[candidate_id] = KnowledgeCandidate(
                candidate_id=candidate_id,
                query_type=query_type,
                representative_query=trigger_record.user_query,
                representative_response=representative_response,
                applicable_types=list(applicable_types),
                evidence_count=len(similar_queries) + 1,
                confidence=trigger_record.confidence,
                first_seen=min(q.timestamp for q in similar_queries + [trigger_record]),
                last_seen=trigger_record.timestamp,
                validation_needed=True,
                supporting_queries=[q.user_query for q in similar_queries + [trigger_record]]
            )
            
            logging.info(f"ìƒˆë¡œìš´ ì§€ì‹ í›„ë³´ ë°œê²¬: {candidate_id} (ì¦ê±°: {len(similar_queries) + 1}ê°œ)")
        else:
            # ê¸°ì¡´ í›„ë³´ ì—…ë°ì´íŠ¸
            candidate = self.knowledge_candidates[candidate_id]
            candidate.evidence_count += 1
            candidate.last_seen = trigger_record.timestamp
            candidate.supporting_queries.append(trigger_record.user_query)
    
    def _classify_query_type(self, query: str) -> str:
        """ì§ˆì˜ ìœ í˜• ë¶„ë¥˜"""
        
        type_patterns = {
            "ìê²©ìš”ê±´": ["ìê²©", "ì¡°ê±´", "ìš”êµ¬ì‚¬í•­", "ê¸°ì¤€", "ëŒ€ìƒ", "í•´ë‹¹"],
            "ì‹ ì²­ì ˆì°¨": ["ë°©ë²•", "ì ˆì°¨", "ì–´ë–»ê²Œ", "ìˆœì„œ", "ë‹¨ê³„", "ì‹ ì²­", "ì ‘ìˆ˜"],
            "ë¹„ìš©ì •ë³´": ["ì–¼ë§ˆ", "ë¹„ìš©", "ê°€ê²©", "ì„ëŒ€ë£Œ", "ë³´ì¦ê¸ˆ", "ìˆ˜ìˆ˜ë£Œ", "ëˆ"],
            "ì¼ì •ì •ë³´": ["ì–¸ì œ", "ê¸°ê°„", "ë§ˆê°", "ì¼ì •", "ì‹œê¸°", "ë‚ ì§œ"],
            "ì œë„ì„¤ëª…": ["ë¬´ì—‡", "ë­", "ì„¤ëª…", "ê°œë…", "ì˜ë¯¸", "ì •ì˜", "ì°¨ì´"]
        }
        
        query_lower = query.lower()
        
        for query_type, keywords in type_patterns.items():
            if any(keyword in query_lower for keyword in keywords):
                return query_type
        
        return "ì¼ë°˜ë¬¸ì˜"
    
    def _select_best_response(self, responses: List[str]) -> str:
        """ê°€ì¥ ì¢‹ì€ ì‘ë‹µ ì„ íƒ"""
        
        if not responses:
            return ""
        
        # ê¸¸ì´ì™€ ì •ë³´ ë°€ë„ë¥¼ ê³ ë ¤í•˜ì—¬ ì„ íƒ
        scored_responses = []
        
        for response in responses:
            score = len(response)  # ê¸¸ì´ ì ìˆ˜
            
            # êµ¬ì²´ì  ì •ë³´ í¬í•¨ ì—¬ë¶€ë¡œ ì¶”ê°€ ì ìˆ˜
            if any(keyword in response for keyword in ["ë§Œ", "ì„¸", "%", "ì›", "ì¼", "ì›”"]):
                score += 100
            
            # êµ¬ì¡°í™”ëœ í‘œí˜„ í¬í•¨ ì—¬ë¶€
            if any(marker in response for marker in ["###", "##", "â€¢", "-", "1.", "2."]):
                score += 50
            
            scored_responses.append((response, score))
        
        # ê°€ì¥ ë†’ì€ ì ìˆ˜ì˜ ì‘ë‹µ ì„ íƒ
        best_response = max(scored_responses, key=lambda x: x[1])[0]
        return best_response

# ==================== ë™ì  ì§€ì‹ ê·¸ë˜í”„ ====================

class DynamicKnowledgeGraph:
    """ë™ì ìœ¼ë¡œ êµ¬ì¶•ë˜ëŠ” ì§€ì‹ ê·¸ë˜í”„"""
    
    def __init__(self, storage_path: str = "./knowledge_graph"):
        self.graph = nx.DiGraph()
        self.node_embeddings = {}
        self.storage_path = storage_path
        self.embeddings = OpenAIEmbeddings(model="text-embedding-3-small")
        
        # ì €ì¥ ë””ë ‰í† ë¦¬ ìƒì„±
        os.makedirs(storage_path, exist_ok=True)
        
        # ê¸°ì¡´ ê·¸ë˜í”„ ë¡œë“œ
        self._load_graph()
        
    def _load_graph(self):
        """ì €ì¥ëœ ê·¸ë˜í”„ ë¡œë“œ"""
        
        graph_file = os.path.join(self.storage_path, "knowledge_graph.pkl")
        embeddings_file = os.path.join(self.storage_path, "node_embeddings.pkl")
        
        try:
            if os.path.exists(graph_file):
                with open(graph_file, 'rb') as f:
                    self.graph = pickle.load(f)
                logging.info(f"ì§€ì‹ ê·¸ë˜í”„ ë¡œë“œ: {len(self.graph.nodes)} ë…¸ë“œ, {len(self.graph.edges)} ì—£ì§€")
            
            if os.path.exists(embeddings_file):
                with open(embeddings_file, 'rb') as f:
                    self.node_embeddings = pickle.load(f)
                    
        except Exception as e:
            logging.error(f"ê·¸ë˜í”„ ë¡œë“œ ì‹¤íŒ¨: {str(e)}")
            self.graph = nx.DiGraph()
            self.node_embeddings = {}
    
    def _save_graph(self):
        """ê·¸ë˜í”„ ì €ì¥"""
        
        try:
            graph_file = os.path.join(self.storage_path, "knowledge_graph.pkl")
            embeddings_file = os.path.join(self.storage_path, "node_embeddings.pkl")
            
            with open(graph_file, 'wb') as f:
                pickle.dump(self.graph, f)
            
            with open(embeddings_file, 'wb') as f:
                pickle.dump(self.node_embeddings, f)
                
            logging.info("ì§€ì‹ ê·¸ë˜í”„ ì €ì¥ ì™„ë£Œ")
            
        except Exception as e:
            logging.error(f"ê·¸ë˜í”„ ì €ì¥ ì‹¤íŒ¨: {str(e)}")
    
    async def update_knowledge_from_patterns(self, 
                                           discovered_patterns: List[CommonPattern]):
        """ë°œê²¬ëœ íŒ¨í„´ì„ ì§€ì‹ ê·¸ë˜í”„ì— í†µí•©"""
        
        for pattern in discovered_patterns:
            await self._integrate_pattern_as_knowledge(pattern)
        
        # ê·¸ë˜í”„ ì €ì¥
        self._save_graph()
    
    async def _integrate_pattern_as_knowledge(self, pattern: CommonPattern):
        """íŒ¨í„´ì„ ì§€ì‹ ë…¸ë“œë¡œ ë³€í™˜í•˜ì—¬ ê·¸ë˜í”„ì— ì¶”ê°€"""
        
        # ì§€ì‹ ë…¸ë“œ ìƒì„±
        knowledge_node = KnowledgeNode(
            node_id=pattern.pattern_id,
            node_type=pattern.pattern_type,
            content=pattern.content,
            confidence_score=pattern.confidence,
            source_count=pattern.frequency,
            validation_count=0,
            created_at=pattern.extracted_at,
            last_updated=pattern.extracted_at,
            applicable_types={pattern.ì²­ì•½ìœ í˜•},
            regional_scope={"ì„œìš¸íŠ¹ë³„ì‹œ"},  # ê¸°ë³¸ê°’
            evidence_sources=pattern.source_notices
        )
        
        # ê·¸ë˜í”„ì— ë…¸ë“œ ì¶”ê°€
        self.graph.add_node(
            pattern.pattern_id,
            **knowledge_node.__dict__
        )
        
        # ë…¸ë“œ ì„ë² ë”© ìƒì„± ë° ì €ì¥
        try:
            content_embedding = await self._get_content_embedding(pattern.content)
            self.node_embeddings[pattern.pattern_id] = content_embedding
        except Exception as e:
            logging.error(f"ì„ë² ë”© ìƒì„± ì‹¤íŒ¨ ({pattern.pattern_id}): {str(e)}")
        
        # ê´€ë ¨ ë…¸ë“œë“¤ê³¼ì˜ ê´€ê³„ ì„¤ì •
        await self._establish_relationships(knowledge_node)
        
        logging.info(f"ì§€ì‹ ë…¸ë“œ ì¶”ê°€: {pattern.pattern_id}")
    
    async def _get_content_embedding(self, content: str) -> np.ndarray:
        """ì»¨í…ì¸  ì„ë² ë”© ìƒì„±"""
        
        try:
            embedding = self.embeddings.embed_query(content)
            return np.array(embedding)
        except Exception as e:
            logging.error(f"ì„ë² ë”© ìƒì„± ì‹¤íŒ¨: {str(e)}")
            return np.zeros(1536)
    
    async def _establish_relationships(self, new_node: KnowledgeNode):
        """ìƒˆ ë…¸ë“œì™€ ê¸°ì¡´ ë…¸ë“œë“¤ ê°„ì˜ ê´€ê³„ ì„¤ì •"""
        
        # ìœ ì‚¬í•œ ë…¸ë“œ ì°¾ê¸°
        similar_nodes = await self._find_similar_nodes(new_node)
        
        for similar_node_id, similarity_score in similar_nodes:
            if similarity_score > 0.7:  # ë†’ì€ ìœ ì‚¬ë„
                # ìœ ì‚¬ ê´€ê³„ ì„¤ì •
                edge = KnowledgeEdge(
                    source_node=new_node.node_id,
                    target_node=similar_node_id,
                    relation_type="similar_to",
                    weight=similarity_score,
                    evidence_count=1,
                    created_at=datetime.now()
                )
                
                self.graph.add_edge(
                    new_node.node_id,
                    similar_node_id,
                    **edge.__dict__
                )
        
        # ë…¼ë¦¬ì  ê´€ê³„ ì¶”ë¡  (ì˜ˆ: ìê²©ìš”ê±´ â†’ ì‹ ì²­ì ˆì°¨)
        logical_relationships = self._infer_logical_relationships(new_node)
        
        for target_node_id, relation_type, confidence in logical_relationships:
            edge = KnowledgeEdge(
                source_node=new_node.node_id,
                target_node=target_node_id,
                relation_type=relation_type,
                weight=confidence,
                evidence_count=1,
                created_at=datetime.now()
            )
            
            self.graph.add_edge(
                new_node.node_id,
                target_node_id,
                **edge.__dict__
            )
    
    async def _find_similar_nodes(self, new_node: KnowledgeNode) -> List[Tuple[str, float]]:
        """ìƒˆ ë…¸ë“œì™€ ìœ ì‚¬í•œ ê¸°ì¡´ ë…¸ë“œë“¤ ì°¾ê¸°"""
        
        similar_nodes = []
        
        if new_node.node_id not in self.node_embeddings:
            return similar_nodes
        
        new_embedding = self.node_embeddings[new_node.node_id]
        
        for node_id, embedding in self.node_embeddings.items():
            if node_id != new_node.node_id:
                similarity = cosine_similarity([new_embedding], [embedding])[0][0]
                if similarity > 0.5:  # ê¸°ë³¸ ì„ê³„ê°’
                    similar_nodes.append((node_id, similarity))
        
        # ìœ ì‚¬ë„ ìˆœìœ¼ë¡œ ì •ë ¬
        similar_nodes.sort(key=lambda x: x[1], reverse=True)
        
        return similar_nodes[:5]  # ìƒìœ„ 5ê°œë§Œ
    
    def _infer_logical_relationships(self, new_node: KnowledgeNode) -> List[Tuple[str, str, float]]:
        """ë…¼ë¦¬ì  ê´€ê³„ ì¶”ë¡ """
        
        relationships = []
        
        # ë…¸ë“œ íƒ€ì… ê¸°ë°˜ ë…¼ë¦¬ì  ê´€ê³„ ì„¤ì •
        logical_flows = {
            ("ìê²©ìš”ê±´", "ì‹ ì²­ì ˆì°¨"): ("requires", 0.8),
            ("ì‹ ì²­ì ˆì°¨", "ë¹„ìš©ì •ë³´"): ("leads_to", 0.7),
            ("ìê²©ìš”ê±´", "ë¹„ìš©ì •ë³´"): ("related_to", 0.6)
        }
        
        for node_id in self.graph.nodes():
            existing_node_data = self.graph.nodes[node_id]
            existing_type = existing_node_data.get('node_type')
            
            # ê°™ì€ ì²­ì•½ ìœ í˜•ì—ë§Œ ì ìš©
            existing_applicable_types = existing_node_data.get('applicable_types', set())
            if not (new_node.applicable_types & existing_applicable_types):
                continue
            
            # ë…¼ë¦¬ì  ê´€ê³„ í™•ì¸
            for (source_type, target_type), (relation, confidence) in logical_flows.items():
                if (new_node.node_type == source_type and existing_type == target_type) or \
                   (new_node.node_type == target_type and existing_type == source_type):
                    relationships.append((node_id, relation, confidence))
        
        return relationships
    
    def validate_knowledge_candidate(self, candidate: KnowledgeCandidate) -> bool:
        """ì§€ì‹ í›„ë³´ì˜ ìœ íš¨ì„± ê²€ì¦"""
        
        validation_criteria = {
            "evidence_threshold": 5,    # ìµœì†Œ 5ë²ˆì˜ ì¦ê±°
            "confidence_threshold": 0.8, # 80% ì´ìƒ ì‹ ë¢°ë„
            "consistency_check": True,   # ê¸°ì¡´ ì§€ì‹ê³¼ì˜ ì¼ê´€ì„±
            "temporal_stability": True   # ì‹œê°„ì  ì•ˆì •ì„± (1ì£¼ì¼ ì´ìƒ)
        }
        
        # 1. ì¦ê±° ì¶©ë¶„ì„± í™•ì¸
        if candidate.evidence_count < validation_criteria["evidence_threshold"]:
            logging.info(f"ì§€ì‹ í›„ë³´ {candidate.candidate_id}: ì¦ê±° ë¶€ì¡± ({candidate.evidence_count})")
            return False
        
        # 2. ì‹ ë¢°ë„ í™•ì¸
        if candidate.confidence < validation_criteria["confidence_threshold"]:
            logging.info(f"ì§€ì‹ í›„ë³´ {candidate.candidate_id}: ì‹ ë¢°ë„ ë¶€ì¡± ({candidate.confidence})")
            return False
        
        # 3. ì‹œê°„ì  ì•ˆì •ì„± í™•ì¸
        if validation_criteria["temporal_stability"]:
            time_span = candidate.last_seen - candidate.first_seen
            if time_span < timedelta(days=7):
                logging.info(f"ì§€ì‹ í›„ë³´ {candidate.candidate_id}: ì‹œê°„ì  ì•ˆì •ì„± ë¶€ì¡±")
                return False
        
        # 4. ê¸°ì¡´ ì§€ì‹ê³¼ì˜ ì¼ê´€ì„± í™•ì¸
        if validation_criteria["consistency_check"]:
            is_consistent = self._check_consistency_with_existing_knowledge(candidate)
            if not is_consistent:
                logging.info(f"ì§€ì‹ í›„ë³´ {candidate.candidate_id}: ê¸°ì¡´ ì§€ì‹ê³¼ ëª¨ìˆœ")
                return False
        
        logging.info(f"ì§€ì‹ í›„ë³´ {candidate.candidate_id}: ê²€ì¦ í†µê³¼")
        return True
    
    def _check_consistency_with_existing_knowledge(self, candidate: KnowledgeCandidate) -> bool:
        """ê¸°ì¡´ ì§€ì‹ê³¼ì˜ ì¼ê´€ì„± í™•ì¸"""
        
        # ë™ì¼í•œ ì§ˆì˜ ìœ í˜•ì˜ ê¸°ì¡´ ë…¸ë“œë“¤ í™•ì¸
        for node_id in self.graph.nodes():
            node_data = self.graph.nodes[node_id]
            
            # ê°™ì€ íƒ€ì…ì´ê³  ê°™ì€ ì ìš© ë²”ìœ„ì¸ ê²½ìš°
            if (node_data.get('node_type') == candidate.query_type and
                set(candidate.applicable_types) & node_data.get('applicable_types', set())):
                
                # ë‚´ìš© ìœ ì‚¬ë„ í™•ì¸
                existing_content = node_data.get('content', '')
                
                # ê°„ë‹¨í•œ í‚¤ì›Œë“œ ê¸°ë°˜ ëª¨ìˆœ í™•ì¸
                if self._check_contradiction(candidate.representative_response, existing_content):
                    return False
        
        return True
    
    def _check_contradiction(self, response1: str, response2: str) -> bool:
        """ë‘ ì‘ë‹µ ê°„ì˜ ëª¨ìˆœ í™•ì¸"""
        
        # ê°„ë‹¨í•œ ëª¨ìˆœ íŒ¨í„´ í™•ì¸
        contradiction_patterns = [
            (r'(\d+)ì„¸\s*ì´ìƒ', r'(\d+)ì„¸\s*ì´í•˜'),  # ë‚˜ì´ ë²”ìœ„ ëª¨ìˆœ
            (r'(\d+)%\s*ì´í•˜', r'(\d+)%\s*ì´ìƒ'),   # ì†Œë“ ê¸°ì¤€ ëª¨ìˆœ
            (r'ë¬´ì£¼íƒ', r'ì£¼íƒ\s*ì†Œìœ '),              # ì£¼íƒ ì†Œìœ  ëª¨ìˆœ
        ]
        
        for pattern1, pattern2 in contradiction_patterns:
            match1 = re.search(pattern1, response1)
            match2 = re.search(pattern2, response2)
            
            if match1 and match2:
                # ìˆ˜ì¹˜ ë¹„êµ (ë‚˜ì´, ì†Œë“ ë“±)
                try:
                    val1 = int(match1.group(1))
                    val2 = int(match2.group(1))
                    
                    # ë…¼ë¦¬ì ìœ¼ë¡œ ëª¨ìˆœë˜ëŠ” ê°’ì¸ì§€ í™•ì¸
                    if abs(val1 - val2) > 20:  # ì„ê³„ê°’
                        return True
                except:
                    pass
        
        return False
    
    async def promote_candidate_to_knowledge(self, candidate: KnowledgeCandidate):
        """ê²€ì¦ëœ í›„ë³´ë¥¼ ì •ì‹ ì§€ì‹ìœ¼ë¡œ ìŠ¹ê²©"""
        
        # ì§€ì‹ ë…¸ë“œ ìƒì„±
        knowledge_node = KnowledgeNode(
            node_id=f"learned_{candidate.candidate_id}",
            node_type=candidate.query_type,
            content=candidate.representative_response,
            confidence_score=candidate.confidence,
            source_count=candidate.evidence_count,
            validation_count=1,
            created_at=candidate.first_seen,
            last_updated=datetime.now(),
            applicable_types=set(candidate.applicable_types),
            regional_scope={"ì„œìš¸íŠ¹ë³„ì‹œ"},
            evidence_sources=candidate.supporting_queries[:5]  # ìµœëŒ€ 5ê°œ
        )
        
        # ê·¸ë˜í”„ì— ì¶”ê°€
        self.graph.add_node(
            knowledge_node.node_id,
            **knowledge_node.__dict__
        )
        
        # ì„ë² ë”© ìƒì„±
        try:
            content_embedding = await self._get_content_embedding(candidate.representative_response)
            self.node_embeddings[knowledge_node.node_id] = content_embedding
        except Exception as e:
            logging.error(f"ì„ë² ë”© ìƒì„± ì‹¤íŒ¨: {str(e)}")
        
        # ê´€ê³„ ì„¤ì •
        await self._establish_relationships(knowledge_node)
        
        # ê·¸ë˜í”„ ì €ì¥
        self._save_graph()
        
        logging.info(f"ì§€ì‹ í›„ë³´ê°€ ì •ì‹ ì§€ì‹ìœ¼ë¡œ ìŠ¹ê²©: {candidate.candidate_id}")

# ==================== ì§€ì‹ ì¶”ë¡  ì—”ì§„ ====================

class KnowledgeInferenceEngine:
    """ì§€ì‹ ê·¸ë˜í”„ ê¸°ë°˜ ì¶”ë¡  ì—”ì§„"""
    
    def __init__(self, knowledge_graph: DynamicKnowledgeGraph):
        self.kg = knowledge_graph
        self.llm = ChatOpenAI(model="gpt-4o-mini", temperature=0.1)
        
    async def answer_with_knowledge_graph(self, 
                                        user_query: str, 
                                        context_notice: Optional[NoticeInfo] = None) -> str:
        """ì§€ì‹ ê·¸ë˜í”„ë¥¼ í™œìš©í•œ ë‹µë³€ ìƒì„±"""
        
        # 1. ê´€ë ¨ ì§€ì‹ ë…¸ë“œ ê²€ìƒ‰
        relevant_nodes = await self._search_relevant_knowledge(user_query, context_notice)
        
        if not relevant_nodes:
            return "ê´€ë ¨ëœ ë³´í¸ì  ì§€ì‹ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. êµ¬ì²´ì ì¸ ê³µê³ ë¬¸ì„ ì„ íƒí•˜ì‹œë©´ ë” ì •í™•í•œ ë‹µë³€ì„ ì œê³µí•  ìˆ˜ ìˆìŠµë‹ˆë‹¤."
        
        # 2. ì§€ì‹ ê·¸ë˜í”„ íƒìƒ‰ìœ¼ë¡œ ì—°ê´€ ì •ë³´ ìˆ˜ì§‘
        extended_knowledge = self._explore_related_knowledge(relevant_nodes)
        
        # 3. ë‹µë³€ ìƒì„±
        response = await self._generate_knowledge_based_response(
            user_query, relevant_nodes, extended_knowledge
        )
        
        return response
    
    async def _search_relevant_knowledge(self, 
                                       query: str, 
                                       context_notice: Optional[NoticeInfo]) -> List[Tuple[str, float]]:
        """ê´€ë ¨ ì§€ì‹ ë…¸ë“œ ê²€ìƒ‰"""
        
        try:
            # ì¿¼ë¦¬ ì„ë² ë”© ìƒì„±
            query_embedding = await self.kg._get_content_embedding(query)
            
            relevant_nodes = []
            
            for node_id in self.kg.graph.nodes():
                node_data = self.kg.graph.nodes[node_id]
                
                # ì»¨í…ìŠ¤íŠ¸ ë§¤ì¹­ (ì²­ì•½ ìœ í˜•ì´ ì¼ì¹˜í•˜ëŠ”ê°€)
                if context_notice and context_notice.ì²­ì•½ìœ í˜•:
                    applicable_types = node_data.get('applicable_types', set())
                    if context_notice.ì²­ì•½ìœ í˜• not in applicable_types:
                        continue
                
                # ì˜ë¯¸ì  ìœ ì‚¬ë„ í™•ì¸
                if node_id in self.kg.node_embeddings:
                    node_embedding = self.kg.node_embeddings[node_id]
                    similarity = cosine_similarity([query_embedding], [node_embedding])[0][0]
                    
                    if similarity > 0.6:  # ì„ê³„ê°’ ì´ìƒ
                        relevant_nodes.append((node_id, similarity))
            
            # ìœ ì‚¬ë„ ìˆœìœ¼ë¡œ ì •ë ¬
            relevant_nodes.sort(key=lambda x: x[1], reverse=True)
            
            return relevant_nodes[:5]  # ìƒìœ„ 5ê°œ
            
        except Exception as e:
            logging.error(f"ì§€ì‹ ê²€ìƒ‰ ì‹¤íŒ¨: {str(e)}")
            return []
    
    def _explore_related_knowledge(self, seed_nodes: List[Tuple[str, float]]) -> Dict[str, List[str]]:
        """ì‹œë“œ ë…¸ë“œë“¤ë¡œë¶€í„° ì—°ê´€ ì§€ì‹ íƒìƒ‰"""
        
        related_knowledge = defaultdict(list)
        
        for node_id, _ in seed_nodes:
            # 1-hop ì´ì›ƒ ë…¸ë“œë“¤ ìˆ˜ì§‘
            try:
                neighbors = list(self.kg.graph.neighbors(node_id))
                
                # ê´€ê³„ ìœ í˜•ë³„ë¡œ ë¶„ë¥˜
                for neighbor in neighbors:
                    if self.kg.graph.has_edge(node_id, neighbor):
                        edge_data = self.kg.graph.edges[node_id, neighbor]
                        relation_type = edge_data.get('relation_type', 'related')
                        
                        if neighbor not in [n[0] for n in seed_nodes]:  # ì¤‘ë³µ ì œê±°
                            related_knowledge[relation_type].append(neighbor)
                            
            except Exception as e:
                logging.error(f"ê´€ë ¨ ì§€ì‹ íƒìƒ‰ ì‹¤íŒ¨ ({node_id}): {str(e)}")
                continue
        
        return dict(related_knowledge)
    
    async def _generate_knowledge_based_response(self, 
                                               query: str, 
                                               relevant_nodes: List[Tuple[str, float]],
                                               extended_knowledge: Dict) -> str:
        """ì§€ì‹ ê¸°ë°˜ ì‘ë‹µ ìƒì„±"""
        
        # ì£¼ìš” ì§€ì‹ ë‚´ìš© ìˆ˜ì§‘
        main_knowledge = []
        total_confidence = 0
        
        for node_id, similarity in relevant_nodes[:3]:  # ìƒìœ„ 3ê°œë§Œ
            try:
                node_data = self.kg.graph.nodes[node_id]
                content = node_data.get('content', '')
                confidence = node_data.get('confidence_score', 0.0)
                node_type = node_data.get('node_type', 'ì¼ë°˜')
                
                # JSON í˜•íƒœì˜ êµ¬ì¡°í™”ëœ ë°ì´í„° íŒŒì‹±
                if content.startswith('{'):
                    try:
                        parsed_content = json.loads(content)
                        formatted_content = self._format_structured_content(parsed_content, node_type)
                        main_knowledge.append(formatted_content)
                    except:
                        main_knowledge.append(content)
                else:
                    main_knowledge.append(content)
                
                total_confidence += confidence
                
            except Exception as e:
                logging.error(f"ì§€ì‹ ë‚´ìš© ìˆ˜ì§‘ ì‹¤íŒ¨ ({node_id}): {str(e)}")
                continue
        
        if not main_knowledge:
            return "ê´€ë ¨ ì§€ì‹ì„ ì°¾ì•˜ì§€ë§Œ ë‚´ìš©ì„ ì½ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤."
        
        # LLMì„ í†µí•œ ìì—°ìŠ¤ëŸ¬ìš´ ì‘ë‹µ ìƒì„±
        try:
            context = "\n".join(main_knowledge)
            
            prompt = f"""
ë‹¹ì‹ ì€ ì£¼íƒ ì²­ì•½ ì „ë¬¸ê°€ì…ë‹ˆë‹¤. ì¶•ì ëœ ë³´í¸ì  ì§€ì‹ì„ ë°”íƒ•ìœ¼ë¡œ ì‚¬ìš©ì ì§ˆë¬¸ì— ë‹µë³€í•´ì£¼ì„¸ìš”.

ì‚¬ìš©ì ì§ˆë¬¸: {query}

ê´€ë ¨ ì§€ì‹:
{context}

ë‹µë³€ ê°€ì´ë“œë¼ì¸:
1. ì§€ì‹ ê·¸ë˜í”„ì—ì„œ í•™ìŠµëœ ë³´í¸ì  ì •ë³´ë¥¼ ë°”íƒ•ìœ¼ë¡œ ë‹µë³€
2. êµ¬ì²´ì ì´ê³  ì‹¤ìš©ì ì¸ ì •ë³´ ì œê³µ
3. ì—¬ëŸ¬ ê³µê³ ë¬¸ì—ì„œ ê³µí†µì ìœ¼ë¡œ ë‚˜íƒ€ë‚˜ëŠ” íŒ¨í„´ ì„¤ëª…
4. ë¶ˆí™•ì‹¤í•œ ë¶€ë¶„ì€ ëª…ì‹œ

ë‹µë³€:
"""
            
            response = self.llm.invoke(prompt)
            generated_response = response.content
            
            # ë©”íƒ€ ì •ë³´ ì¶”ê°€
            avg_confidence = total_confidence / len(relevant_nodes) if relevant_nodes else 0
            metadata = f"\n\nğŸ“Š **ì§€ì‹ ì •ë³´**\n"
            metadata += f"â€¢ ê´€ë ¨ ì§€ì‹: {len(relevant_nodes)}ê°œ\n"
            metadata += f"â€¢ í‰ê·  ì‹ ë¢°ë„: {avg_confidence:.1%}\n"
            metadata += f"â€¢ í•™ìŠµ ê¸°ë°˜: ì—¬ëŸ¬ ê³µê³ ë¬¸ì—ì„œ ë°œê²¬ëœ ê³µí†µ íŒ¨í„´\n"
            
            # ì—°ê´€ ì •ë³´ ì¶”ê°€
            if extended_knowledge:
                metadata += f"â€¢ ì—°ê´€ ì§€ì‹: {sum(len(nodes) for nodes in extended_knowledge.values())}ê°œ\n"
            
            return generated_response + metadata
            
        except Exception as e:
            logging.error(f"ì‘ë‹µ ìƒì„± ì‹¤íŒ¨: {str(e)}")
            
            # í´ë°±: ì§ì ‘ ì§€ì‹ ë‚´ìš© ë°˜í™˜
            response_parts = ["ğŸ“š **ê´€ë ¨ ì§€ì‹**\n"]
            response_parts.extend(main_knowledge)
            
            return "\n".join(response_parts)
    
    def _format_structured_content(self, content_dict: Dict, node_type: str) -> str:
        """êµ¬ì¡°í™”ëœ ë‚´ìš©ì„ ìì—°ìŠ¤ëŸ½ê²Œ í¬ë§·íŒ…"""
        
        if node_type == "ìê²©ìš”ê±´":
            formatted = "**ìê²© ìš”ê±´:**\n"
            for key, value in content_dict.items():
                if key == "ì—°ë ¹":
                    formatted += f"â€¢ ì—°ë ¹: {value}\n"
                elif key == "ì†Œë“":
                    formatted += f"â€¢ ì†Œë“ ê¸°ì¤€: ì›”í‰ê· ì†Œë“ {value}\n"
                elif key == "ê±°ì£¼":
                    formatted += f"â€¢ ê±°ì£¼ ìš”ê±´: {value} ê±°ì£¼ ë˜ëŠ” ì§ì¥\n"
                elif key == "ë¬´ì£¼íƒ":
                    formatted += f"â€¢ ì£¼íƒ ì†Œìœ : {value}\n"
                else:
                    formatted += f"â€¢ {key}: {value}\n"
                    
        elif node_type == "ë¹„ìš©ì •ë³´":
            formatted = "**ë¹„ìš© ì •ë³´:**\n"
            for key, value in content_dict.items():
                if "ì„ëŒ€ë£Œ" in key:
                    formatted += f"â€¢ ì„ëŒ€ë£Œ: {value}\n"
                elif "ë³´ì¦ê¸ˆ" in key:
                    formatted += f"â€¢ ë³´ì¦ê¸ˆ: {value}\n"
                else:
                    formatted += f"â€¢ {key}: {value}\n"
                    
        elif node_type == "ì‹ ì²­ì ˆì°¨":
            formatted = "**ì‹ ì²­ ì ˆì°¨:**\n"
            if "ì ˆì°¨" in content_dict:
                for i, step in enumerate(content_dict["ì ˆì°¨"], 1):
                    formatted += f"{i}. {step}\n"
            else:
                for key, value in content_dict.items():
                    formatted += f"â€¢ {key}: {value}\n"
        else:
            # ê¸°ë³¸ í¬ë§·
            formatted = f"**{node_type}:**\n"
            for key, value in content_dict.items():
                formatted += f"â€¢ {key}: {value}\n"
        
        return formatted
